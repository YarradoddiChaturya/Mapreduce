{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1**"
      ],
      "metadata": {
        "id": "vn8wQslyY8BD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Acquire Data:** I have downloaded the book form the link\n",
        "https://ztcprep.com/library/story/Harry_Potter/Harry_Potter_(www.ztcprep.com).pdf"
      ],
      "metadata": {
        "id": "E9lxQNzCYiyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2**"
      ],
      "metadata": {
        "id": "StoLdURnZe1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extract Data**"
      ],
      "metadata": {
        "id": "wVJeTI92aaGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Select the Book:** My birth month is June (6), and thus according to the instructions, the book to be utilized is Book 6 (Harry Potter and the Half-Blood Prince).\n",
        "2.**file1.txt:** my birthday is June 17, so file1.txt start from page 17 of Book 6 and pull 10 pages.\n",
        "3.**file2.txt:** my birth year is 2001, so the page number is 101.Extract 10 pages from Book 6 from page 101"
      ],
      "metadata": {
        "id": "jQynL6NEaaCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**import the libraries**"
      ],
      "metadata": {
        "id": "o20czxp2bXLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker\n",
        "!pip install PyPDF2\n",
        "!pip install fpdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHHNbWodgJss",
        "outputId": "d22bdde3-0fc9-46c7-dd1f-7e02db3791a2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.11/dist-packages (0.8.2)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.11/dist-packages (1.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader  # Extract text from PDF\n",
        "import re  # Regular expressions for text processing\n",
        "import pandas as pd  # Data manipulation and storage\n",
        "from collections import Counter  # Count occurrences of words\n",
        "from spellchecker import SpellChecker  # Identify non-English words\n",
        "from fpdf import FPDF  # Generate PDF report\n",
        "import matplotlib.pyplot as plt  # Generate graphs\n"
      ],
      "metadata": {
        "id": "JHNVmjWibQIi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_PATH = \"/content/Harry_Potter_(www.ztcprep.com).pdf\"\n",
        "FILE1_PATH = \"file1.txt\"\n",
        "FILE2_PATH = \"file2.txt\"\n",
        "\n",
        "# Define the book and pages based on birthdate 17/06/2001\n",
        "BIRTH_MONTH = 6  # June\n",
        "BIRTH_DATE = 17  # Day\n",
        "BIRTH_YEAR = 2001  # Year\n",
        "\n",
        "BOOK_NUMBER = 6  # Using Half-Blood Prince\n",
        "PAGE1_START = BIRTH_DATE  # Extract pages 17-26\n",
        "PAGE2_START = 101  # Extract pages 101-110\n",
        "\n",
        "# Function to extract text\n",
        "def extract_text_from_pdf(pdf_path, pages):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    extracted_text = []\n",
        "    for p in pages:\n",
        "        if p <= len(reader.pages):\n",
        "            text = reader.pages[p - 1].extract_text()\n",
        "            if text:\n",
        "                extracted_text.append(text)\n",
        "    return \"\\n\".join(extracted_text)\n",
        "\n",
        "# Define pages\n",
        "pages_file1 = list(range(PAGE1_START, PAGE1_START + 10))\n",
        "pages_file2 = list(range(PAGE2_START, PAGE2_START + 10))\n",
        "\n",
        "# Extract text\n",
        "text_file1 = extract_text_from_pdf(PDF_PATH, pages_file1)\n",
        "text_file2 = extract_text_from_pdf(PDF_PATH, pages_file2)\n",
        "\n",
        "# Save text to files\n",
        "with open(FILE1_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text_file1)\n",
        "with open(FILE2_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text_file2)\n",
        "\n",
        "print(f\"Extracted text saved to {FILE1_PATH} and {FILE2_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwukMnBZYhI8",
        "outputId": "05c0c2aa-8399-47ce-a2d9-bd7544a5cfc4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted text saved to file1.txt and file2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3**"
      ],
      "metadata": {
        "id": "GoqIvZxelyAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write Python code and use MapReduct to count occurrences of each word in the first text file (file.txt). How many times each word is repeated?\n",
        "\n"
      ],
      "metadata": {
        "id": "7YH1e2y5b_fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "FILE1_PATH = \"file1.txt\"\n",
        "\n",
        "# Function to tokenize text\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words\n",
        "\n",
        "# Load text file\n",
        "with open(FILE1_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    text_file1 = f.read()\n",
        "\n",
        "# Count word occurrences\n",
        "words_file1 = tokenize(text_file1)\n",
        "word_counts_file1 = Counter(words_file1)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_file1 = pd.DataFrame(word_counts_file1.items(), columns=[\"Word\", \"Count\"]).sort_values(by=\"Count\", ascending=False)\n",
        "\n",
        "# Save word count to CSV\n",
        "df_file1.to_csv(\"word_count.csv\", index=False)\n",
        "\n",
        "print(\"\\nWord Count from file1.txt (All words):\")\n",
        "print(df_file1.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9WRRN-TYhFS",
        "outputId": "21bfdfb3-7608-4c94-c785-f6e776218482"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Count from file1.txt (All words):\n",
            "        Word  Count\n",
            "         the     64\n",
            "          he     43\n",
            "           a     40\n",
            "          to     36\n",
            "         and     34\n",
            "          it     34\n",
            "         was     30\n",
            "           i     26\n",
            "           s     25\n",
            "           t     23\n",
            "  dumbledore     23\n",
            "         you     19\n",
            "         his     19\n",
            "   professor     18\n",
            "          of     18\n",
            "          on     17\n",
            "         all     17\n",
            "  mcgonagall     16\n",
            "         she     16\n",
            "        that     16\n",
            "        said     15\n",
            "         had     14\n",
            "          be     14\n",
            "          in     14\n",
            "        know     13\n",
            "          at     13\n",
            "           y     13\n",
            "       harry     13\n",
            "          re     12\n",
            "          ou     12\n",
            "         for     12\n",
            "         but     12\n",
            "           e     11\n",
            "        have     11\n",
            "        they     11\n",
            "         him     11\n",
            "          as     10\n",
            "         cat     10\n",
            "     ztcprep     10\n",
            "      potter     10\n",
            "         www     10\n",
            "         com     10\n",
            "         can      9\n",
            "         who      9\n",
            "         her      9\n",
            "        been      8\n",
            "        name      7\n",
            "         out      7\n",
            "      saying      7\n",
            "           d      7\n",
            "    oldemort      6\n",
            "      street      6\n",
            "           j      6\n",
            "           k      6\n",
            "      people      6\n",
            "     rowling      6\n",
            "         are      6\n",
            "        here      6\n",
            "           w      6\n",
            "       stone      6\n",
            "           v      6\n",
            "         not      6\n",
            "           p      6\n",
            "philosophers      6\n",
            "          so      6\n",
            "        this      6\n",
            "          ve      6\n",
            "       never      6\n",
            "         day      6\n",
            "          if      6\n",
            "        were      6\n",
            "           g      6\n",
            "        didn      5\n",
            "        eyes      5\n",
            "   something      5\n",
            "         did      5\n",
            "       lemon      5\n",
            "        what      5\n",
            "        even      5\n",
            "          no      5\n",
            "        only      5\n",
            "        very      5\n",
            "      couldn      5\n",
            "        down      5\n",
            "      little      5\n",
            "        will      5\n",
            "        back      4\n",
            "         how      4\n",
            "           f      4\n",
            "        gone      4\n",
            "          up      4\n",
            "     looking      4\n",
            "          me      4\n",
            "        them      4\n",
            "          my      4\n",
            "        much      4\n",
            "      reason      4\n",
            "      seemed      4\n",
            "        went      4\n",
            "       going      4\n",
            "    everyone      4\n",
            "        wall      4\n",
            "        kill      4\n",
            "         why      4\n",
            "       about      4\n",
            "          by      4\n",
            "      though      4\n",
            "        half      4\n",
            "       cloak      4\n",
            "        long      3\n",
            "       which      3\n",
            "         put      3\n",
            "          oh      3\n",
            "      pocket      3\n",
            "         way      3\n",
            "        head      3\n",
            "        next      3\n",
            "        told      3\n",
            "        when      3\n",
            "     believe      3\n",
            "     clicked      3\n",
            "     wearing      3\n",
            "        seen      3\n",
            "         two      3\n",
            "        like      3\n",
            "         boy      3\n",
            "         now      3\n",
            "         one      3\n",
            "      around      3\n",
            "        just      3\n",
            "    watching      3\n",
            "      letter      3\n",
            "         man      3\n",
            "      famous      3\n",
            "        into      3\n",
            "       think      3\n",
            "      looked      3\n",
            "     suppose      3\n",
            "         has      3\n",
            "        drop      3\n",
            "        from      3\n",
            "          ll      3\n",
            "     because      3\n",
            "       albus      3\n",
            "         don      2\n",
            "       watch      2\n",
            "           1      2\n",
            "       seems      2\n",
            "        aunt      2\n",
            "         our      2\n",
            "         too      2\n",
            "       uncle      2\n",
            " disappeared      2\n",
            "       would      2\n",
            "         son      2\n",
            "        come      2\n",
            "       haven      2\n",
            "     exactly      2\n",
            "        last      2\n",
            "     glasses      2\n",
            "        mean      2\n",
            "       woman      2\n",
            "      rather      2\n",
            "        live      2\n",
            "     instead      2\n",
            "     explain      2\n",
            "        tell      2\n",
            "      turned      2\n",
            "     written      2\n",
            "      muggle      2\n",
            "      rumors      2\n",
            "      moment      2\n",
            "        dear      2\n",
            "        stif      2\n",
            "       years      2\n",
            "          us      2\n",
            "       stars      2\n",
            "       knows      2\n",
            "    shooting      2\n",
            "        owls      2\n",
            "        dark      2\n",
            "         any      2\n",
            "      notice      2\n",
            "        call      2\n",
            "       sense      2\n",
            "     muggles      2\n",
            "       drops      2\n",
            "     reached      2\n",
            "       right      2\n",
            "       sharp      2\n",
            "        snif      2\n",
            "        true      2\n",
            "        must      2\n",
            " celebrating      2\n",
            "      eleven      2\n",
            "      really      2\n",
            "          is      2\n",
            "        find      2\n",
            "     sitting      2\n",
            "     potters      2\n",
            "        lily      2\n",
            "       james      2\n",
            "  frightened      2\n",
            "       after      2\n",
            "      before      2\n",
            "        look      2\n",
            "     realize      2\n",
            "      inside      2\n",
            "    appeared      2\n",
            "      silver      2\n",
            "       boots      2\n",
            "    suddenly      2\n",
            "       where      2\n",
            "         its      2\n",
            "       their      2\n",
            "        left      2\n",
            "       found      2\n",
            "        seem      2\n",
            "       until      2\n",
            "       outer      2\n",
            "        lamp      2\n",
            "        moon      2\n",
            "  spectacles      2\n",
            "        with      2\n",
            "      ground      2\n",
            "  everything      2\n",
            "      window      2\n",
            "        hair      2\n",
            "         see      2\n",
            "      enough      2\n",
            "       being      2\n",
            "      number      2\n",
            "        four      2\n",
            "     nothing      2\n",
            "       known      2\n",
            "        able      2\n",
            "      wouldn      2\n",
            "    choosing      1\n",
            "        ever      1\n",
            "     judging      1\n",
            "    narrowed      1\n",
            "     however      1\n",
            "     another      1\n",
            "      privet      1\n",
            "       tried      1\n",
            "      answer      1\n",
            "     pressed      1\n",
            "    trembled      1\n",
            "      gasped      1\n",
            "       voice      1\n",
            "       night      1\n",
            "       bowed      1\n",
            "      hollow      1\n",
            "         old      1\n",
            "          13      1\n",
            "        thin      1\n",
            "     heavily      1\n",
            "       rumor      1\n",
            "    shoulder      1\n",
            "        tall      1\n",
            "      patted      1\n",
            "       drive      1\n",
            "        dead      1\n",
            "        want      1\n",
            "      godric      1\n",
            "        such      1\n",
            "    whatever      1\n",
            "        shot      1\n",
            "      earmuf      1\n",
            "         new      1\n",
            "       liked      1\n",
            "           9      1\n",
            "     pomfrey      1\n",
            "       madam      1\n",
            "       since      1\n",
            "     blushed      1\n",
            "       lucky      1\n",
            "         use      1\n",
            "       noble      1\n",
            "        well      1\n",
            "      powers      1\n",
            "      calmly      1\n",
            "     flatter      1\n",
            "          12      1\n",
            "        tuck      1\n",
            "          fs      1\n",
            "      flying      1\n",
            "       plain      1\n",
            "        both      1\n",
            "       stare      1\n",
            "    piercing      1\n",
            "       power      1\n",
            "       fixed      1\n",
            "         nor      1\n",
            "     neither      1\n",
            "        hard      1\n",
            "        cold      1\n",
            "     waiting      1\n",
            "        real      1\n",
            "     discuss      1\n",
            "     anxious      1\n",
            "        most      1\n",
            "       point      1\n",
            "       beard      1\n",
            "     stopped      1\n",
            "     finally      1\n",
            "          or      1\n",
            "        stop      1\n",
            "     somehow      1\n",
            "       these      1\n",
            "    repeated      1\n",
            "      corner      1\n",
            "       older      1\n",
            "      firmly      1\n",
            "       place      1\n",
            "        best      1\n",
            "      sweets      1\n",
            "   screaming      1\n",
            "      mother      1\n",
            "     kicking      1\n",
            "         saw      1\n",
            "         got      1\n",
            "        less      1\n",
            "    pointing      1\n",
            "        feet      1\n",
            "          14      1\n",
            "     jumping      1\n",
            "     faintly      1\n",
            "  understand      1\n",
            "    silently      1\n",
            "       moved      1\n",
            "    remember      1\n",
            "         won      1\n",
            "        talk      1\n",
            "        walk      1\n",
            "        turn      1\n",
            "         top      1\n",
            "        over      1\n",
            "   seriously      1\n",
            "       world      1\n",
            "       child      1\n",
            "       every      1\n",
            "       books      1\n",
            "       there      1\n",
            "      future      1\n",
            "       today      1\n",
            "   surprised      1\n",
            "      legend      1\n",
            "       cried      1\n",
            "     thought      1\n",
            "       broke      1\n",
            "       great      1\n",
            "     beneath      1\n",
            "      dabbed      1\n",
            "handkerchief      1\n",
            "        lace      1\n",
            "      pulled      1\n",
            "         may      1\n",
            "       guess      1\n",
            "     survive      1\n",
            "      heaven      1\n",
            "         dif      1\n",
            "      things      1\n",
            "  astounding      1\n",
            "      killed      1\n",
            "        done      1\n",
            "    faltered      1\n",
            "      glumly      1\n",
            "      nodded      1\n",
            "        gave      1\n",
            "        took      1\n",
            "      family      1\n",
            "      golden      1\n",
            "      popped      1\n",
            "       bring      1\n",
            "        tail      1\n",
            "      places      1\n",
            "    twitched      1\n",
            "          es      1\n",
            "        late      1\n",
            "      hagrid      1\n",
            "        made      1\n",
            "        edge      1\n",
            "      moving      1\n",
            "     planets      1\n",
            "     numbers      1\n",
            "       hands      1\n",
            "      twelve      1\n",
            "         odd      1\n",
            "    examined      1\n",
            "      ferent      1\n",
            "  unsticking      1\n",
            "    admiring      1\n",
            "       times      1\n",
            " impatiently      1\n",
            "        held      1\n",
            "         air      1\n",
            "         yes      1\n",
            "     nearest      1\n",
            "     angrily      1\n",
            "         fed      1\n",
            "     parties      1\n",
            "      feasts      1\n",
            "       dozen      1\n",
            "      passed      1\n",
            "         pop      1\n",
            "       again      1\n",
            "       could      1\n",
            "   flickered      1\n",
            "    darkness      1\n",
            "       welve      1\n",
            "        open      1\n",
            "         bit      1\n",
            "        more      1\n",
            "        room      1\n",
            "       amuse      1\n",
            "         ell      1\n",
            "    chuckled      1\n",
            "    muttered      1\n",
            "      should      1\n",
            "      flocks      1\n",
            "       heard      1\n",
            "      living      1\n",
            "     careful      1\n",
            "    dursleys      1\n",
            "      jerked      1\n",
            "        news      1\n",
            "   cigarette      1\n",
            "     noticed      1\n",
            "     lighter      1\n",
            "     flicked      1\n",
            "       brick      1\n",
            "         fly      1\n",
            "  completely      1\n",
            "      lights      1\n",
            "    markings      1\n",
            "       shape      1\n",
            "      square      1\n",
            "    anything      1\n",
            "      severe      1\n",
            "   happening      1\n",
            "     smiling      1\n",
            "    pavement      1\n",
            "     slipped      1\n",
            "       tabby      1\n",
            "       smile      1\n",
            "         set      1\n",
            "          10      1\n",
            "      toward      1\n",
            "      seeing      1\n",
            "       fancy      1\n",
            "       spoke      1\n",
            "     dursley      1\n",
            "         mrs      1\n",
            "        eyed      1\n",
            "        fled      1\n",
            "         sit      1\n",
            "       whole      1\n",
            "        tiny      1\n",
            "   pinpricks      1\n",
            "       asked      1\n",
            "    distance      1\n",
            "      anyone      1\n",
            "         ruf      1\n",
            "          an      1\n",
            "  distinctly      1\n",
            "         bun      1\n",
            "       tight      1\n",
            "       drawn      1\n",
            "       black      1\n",
            "       beady      1\n",
            "     emerald      1\n",
            "       sight      1\n",
            "      stupid      1\n",
            " exasperated      1\n",
            "        nose      1\n",
            "    sensible      1\n",
            "      surely      1\n",
            "         say      1\n",
            "      coldly      1\n",
            "       thank      1\n",
            "        fond      1\n",
            "           m      1\n",
            "       sweet      1\n",
            "        kind      1\n",
            "        blue      1\n",
            "       light      1\n",
            "        care      1\n",
            "        ould      1\n",
            "    thankful      1\n",
            "   certainly      1\n",
            "      bright      1\n",
            "   sparkling      1\n",
            "      person      1\n",
            "    yourself      1\n",
            "     buckled      1\n",
            "   confusing      1\n",
            "    sounding      1\n",
            "        belt      1\n",
            "       robes      1\n",
            "      purple      1\n",
            "       swept      1\n",
            "        keep      1\n",
            "          we      1\n",
            "        gets      1\n",
            "    nonsense      1\n",
            "         sat      1\n",
            "    flinched      1\n",
            "        high      1\n",
            "      heeled      1\n",
            "      proper      1\n",
            "    persuade      1\n",
            "      trying      1\n",
            "      behind      1\n",
            "     crooked      1\n",
            "       bound      1\n",
            "      broken      1\n",
            "        lose      1\n",
            "   irritably      1\n",
            "   rummaging      1\n",
            "   celebrate      1\n",
            "    precious      1\n",
            "      gently      1\n",
            "     watched      1\n",
            "       blame      1\n",
            "       still      1\n",
            "     staring      1\n",
            "       other      1\n",
            "      diggle      1\n",
            "     dedalus      1\n",
            "         bet      1\n",
            "         end      1\n",
            "        kent      1\n",
            "        some      1\n",
            "        busy      1\n",
            "       heads      1\n",
            "   unwelcome      1\n",
            "       threw      1\n",
            "       least      1\n",
            "       thing      1\n",
            "        fine      1\n",
            "       twice      1\n",
            "      hoping      1\n",
            "      glance      1\n",
            "    sideways      1\n",
            "    swapping      1\n",
            "   downright      1\n",
            "     clothes      1\n",
            "     arrived      1\n",
            "     dressed      1\n",
            "    daylight      1\n",
            "       broad      1\n",
            "     streets      1\n",
            "    careless      1\n",
            "      better      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4**"
      ],
      "metadata": {
        "id": "_3kOzAgvl5Ki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. From the second text file (file2.txt), write Python code and use MapReduct to count how many times non-English words (names, places, spells etc.) were used. List those words and how many times each was repeated."
      ],
      "metadata": {
        "id": "FJMLVDTAkrBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FILE2_PATH = \"file2.txt\"\n",
        "OUTPUT_CSV = \"non_english_words.csv\"\n",
        "\n",
        "# Initialize SpellChecker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Function to tokenize text\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words\n",
        "\n",
        "# Load text from file2.txt\n",
        "with open(FILE2_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    text_file2 = f.read()\n",
        "\n",
        "# Tokenize words\n",
        "words_file2 = tokenize(text_file2)\n",
        "\n",
        "# Identify non-English words using SpellChecker\n",
        "non_english_words = [word for word in words_file2 if word not in spell]\n",
        "\n",
        "# Count occurrences of non-English words\n",
        "non_english_word_counts = Counter(non_english_words)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_file2 = pd.DataFrame(non_english_word_counts.items(), columns=[\"Non-English Word\", \"Count\"]).sort_values(by=\"Count\", ascending=False)\n",
        "\n",
        "# Save results\n",
        "df_file2.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "print(\"\\nNon-English Words from file2.txt (All words):\")\n",
        "print(df_file2.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxQuBhlIYg6s",
        "outputId": "3e1fb1fd-7e32-42b2-81b4-61bca1073821"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Non-English Words from file2.txt (All words):\n",
            "Non-English Word  Count\n",
            "          hagrid     27\n",
            "             ter     19\n",
            "             www     10\n",
            "         ztcprep     10\n",
            "             yeh     10\n",
            "              ll      7\n",
            "           ernon      6\n",
            "            didn      6\n",
            "       gringotts      5\n",
            "              ap      3\n",
            "            stuf      3\n",
            "              ve      3\n",
            "            hadn      3\n",
            "           albus      2\n",
            "             eah      2\n",
            "          gettin      2\n",
            "          izards      2\n",
            "            wasn      2\n",
            "           knuts      2\n",
            "       deliverin      1\n",
            "              69      1\n",
            "           payin      1\n",
            "             teh      1\n",
            "              mm      1\n",
            "          wouldn      1\n",
            "              70      1\n",
            "            cept      1\n",
            "         fetchin      1\n",
            "       everythin      1\n",
            "              68      1\n",
            "          meself      1\n",
            "            ther      1\n",
            "          diagon      1\n",
            "              67      1\n",
            "              ou      1\n",
            "              66      1\n",
            "            aren      1\n",
            "         speakin      1\n",
            "         shouldn      1\n",
            "              65      1\n",
            "           insul      1\n",
            "            ying      1\n",
            "         dumbled      1\n",
            "            goin      1\n",
            "          muggle      1\n",
            "              64      1\n",
            "          pposed      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5:** PDF Extraction"
      ],
      "metadata": {
        "id": "CY7i7fhlls2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths\n",
        "OUTPUT_WORD_COUNT = \"word_count.csv\"\n",
        "OUTPUT_NON_ENGLISH = \"non_english_words.csv\"\n",
        "OUTPUT_PDF = \"MapReduce_Report.pdf\"\n",
        "\n",
        "# Load data\n",
        "df_file1 = pd.read_csv(OUTPUT_WORD_COUNT)\n",
        "df_file2 = pd.read_csv(OUTPUT_NON_ENGLISH)\n",
        "\n",
        "# Initialize PDF\n",
        "pdf = FPDF()\n",
        "pdf.set_auto_page_break(auto=True, margin=15)\n",
        "pdf.add_page()\n",
        "\n",
        "pdf.set_font(\"Arial\", size=14)\n",
        "pdf.cell(200, 10, \"MapReduce Word Analysis Report\", ln=True, align=\"C\")\n",
        "pdf.ln(10)\n",
        "\n",
        "# Word Count Section\n",
        "pdf.set_font(\"Arial\", size=12)\n",
        "pdf.cell(200, 10, \"Word Count Analysis from file1.txt\", ln=True, align=\"L\")\n",
        "pdf.ln(5)\n",
        "\n",
        "for index, row in df_file1.iterrows():  # Include all words\n",
        "    pdf.cell(200, 10, f\"{row['Word']} - {row['Count']}\", ln=True)\n",
        "\n",
        "pdf.ln(10)\n",
        "\n",
        "# Non-English Words Section\n",
        "pdf.cell(200, 10, \"Non-English Words from file2.txt\", ln=True, align=\"L\")\n",
        "pdf.ln(5)\n",
        "\n",
        "for index, row in df_file2.iterrows():  # Include all non-English words\n",
        "    pdf.cell(200, 10, f\"{row['Non-English Word']} - {row['Count']}\", ln=True)\n",
        "\n",
        "# Save PDF\n",
        "pdf.output(OUTPUT_PDF)\n",
        "print(f\"\\nPDF Report saved as: {OUTPUT_PDF}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ-xEaTJft9p",
        "outputId": "c000cc9f-7386-4959-8ef1-8b3578fbcd86"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "PDF Report saved as: MapReduce_Report.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xoqGVeSykXii"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}